title: çˆ¬å–çŒ«é€”é¹°ç½‘ç«™ä¸Šæ‰€æœ‰ä¸­å›½æ™¯ç‚¹çš„æ•°æ®

categories: Web

----

**å…ˆè¯´ç»“æœ**

- çˆ¬å–äº†ğŸ¦‰çŒ«é€”é¹°ç½‘ç«™ä¸Š**34,237**ä¸ªæ™¯ç‚¹çš„æ‰€æœ‰ä¸­æ–‡è¯„è®ºå…±**25,165**æ¡

  ğŸ˜åŸºæœ¬ä¸Šæˆ‘æ•¢è‚¯å®šæ™¯ç‚¹ä¸€å®šæ˜¯å¾ˆå…¨å¾ˆå…¨çš„(è´¹äº†è€å¤§åŠ²çš„); 

  â˜¹ï¸è¯„è®ºæˆ‘æŸ¥äº†å‡ ä¸ªæ™¯ç‚¹,åŸºæœ¬ä¹Ÿéƒ½æ˜¯å…¨çš„. ä½†æ˜¯æˆ‘æ²¡æœ‰IPæ± ,æ‰€ä»¥åæ¥è¢«åçˆ¬äº†ğŸ˜­ä½†æˆ‘è§‰å¾—ä»£ç æ˜¯æ²¡é—®é¢˜çš„,ç†è®ºä¸Šè¯´èƒ½çˆ¬åˆ°æ‰€æœ‰ä¸­æ–‡è¯„è®º

**æºä»£ç **è¯·è§ [](https://blog.csdn.net/u010978757/article/details/83409571)

**è®¿é—®æˆ‘çš„æ•°æ®åº“** 

- IP: 47.93.238.102
- Port: 3306
- MySQLè´¦å·: dsjk 
- MySQLå¯†ç : dsjk

- Database: scrapy_sies
- Table: sites_all_url_copy4 (ä¸­å›½æ‰€æœ‰æ™¯ç‚¹)
- Table: sites_all_review (æ‰€æœ‰ä¸­æ–‡æ™¯ç‚¹è¯„è®º)



---

# ç½‘ç«™åˆ†æ

æœ¬æ¥ä»¥ä¸ºä» `ä¸­å›½`->`æ™¯ç‚¹`è¿™ä¸ªé¡µé¢<https://www.tripadvisor.cn/Attractions-g294211-Activities-China.html>ä¸‹åº”è¯¥èƒ½æ‰¾åˆ°ä¸€ä¸ªæ€»è¡¨,ç›´æ¥çˆ¬å–å³å¯,ä½†æ˜¯çœ‹äº†ä¸€ä¸‹æ‰å‘ç°å¹¶æ²¡æœ‰è¿™æ ·ä¸€ä¸ªè¡¨,è€Œä¸”ç»™å‡ºçš„åˆ†ç±»æ•°æ®ååˆ†ä¸å…¨,æ€»å…±æ‰296ä¸ªæ™¯ç‚¹(åº”è¯¥éƒ½æ˜¯ç²¾åæ™¯ç‚¹å§...)æ‰€ä»¥è‚¯å®šä¸èƒ½æƒ³ç€ä¸€æ¬¡æŠŠæ‰€æœ‰æ™¯ç‚¹çˆ¬å…¨äº†

æ‰€ä»¥æˆ‘æ‰“ç®—é‡‡ç”¨çš„æ–¹æ¡ˆæ˜¯,å…ˆä»`ä¸­å›½`->`ç›®çš„åœ°`é¡µé¢<https://www.tripadvisor.cn/Tourism-g294211-China-Vacations.html>æŠŠä¸­å›½æ‰€æœ‰ç›®çš„åœ°(æŒ‰æ’è¡Œ)çˆ¬ä¸‹æ¥,å†æŒ‰ç…§ç›®çš„åœ°æŠŠå½“åœ°æ‰€æœ‰æ™¯ç‚¹çˆ¬å–ä¸‹æ¥,ä»¥é’å²›ä¸ºä¾‹,å°±æ˜¯å…¶è¿™ä¸ªé¡µé¢<https://www.tripadvisor.cn/Attractions-g297458-Activities-Qingdao_Shandong.html#ATTRACTION_SORT_WRAPPER>(åŒæ ·æ˜¯æŒ‰æ’è¡Œ)

æ¨æµ‹è¿™æ ·åº”è¯¥èƒ½çˆ¬åˆ°æ¯”è¾ƒå®Œæ•´çš„æ•°æ®äº†

## çˆ¬å–æ‰€æœ‰ç›®çš„åœ°é“¾æ¥

ä¾ç„¶æ˜¯è¿™ä¸ªé“¾æ¥<https://www.tripadvisor.cn/TourismChildrenAjax?geo=294211&offset=11&desktop=true>,åœ¨`å—æ¬¢è¿çš„ç›®çš„åœ°`æ ‡ç­¾ä¸‹æœ‰ä¸ª`ä¸­å›½çš„çƒ­é—¨ç›®çš„åœ°`æ’è¡Œæ¦œ,è™½ç„¶æ¦œå•åªæ˜¾ç¤ºäº†éƒ¨åˆ†å†…å®¹,ä½†å¯ä»¥ç‚¹å‡»`æŸ¥çœ‹æ›´å¤šä¸­å›½çƒ­é—¨ç›®çš„åœ°`åŠ è½½,æˆ‘è¯•äº†ä¸€ä¸‹,è²Œä¼¼å¾ˆå¤šå¾ˆå¤šé¡µ,èƒ½ä¸€ç›´åŠ è½½ä¸‹å»,ç›®æµ‹æ˜¯åŒ…å«äº†æ‰€æœ‰ç›®çš„åœ°çš„

æ‰“å¼€Chromeçš„`å¼€å‘è€…å·¥å…·`,ç‚¹å‡»`æŸ¥çœ‹æ›´å¤šä¸­å›½çƒ­é—¨ç›®çš„åœ°`æŸ¥çœ‹é¡µé¢å‘é€çš„Request,å‘ç°æ˜¯é€šè¿‡

![]()

è¿™ä¸ªè¯·æ±‚ä¸­çš„`offset`å€¼åœ¨åŠ è½½æ–°çš„å†…å®¹çš„

<https://www.tripadvisor.cn/TourismChildrenAjax?geo=294211&offset=1&desktop=true>

ç”¨æµè§ˆå™¨ç›´æ¥æ‰“å¼€è¿™ä¸ªé“¾æ¥,å¯ä»¥å‘ç°,`offset`ä¸º1æ—¶,åŠ è½½çš„å°±æ˜¯ç¬¬ä¸ƒåä¹‹åçš„ç›®çš„åœ°,æ‰€ä»¥ä¿®æ”¹offsetå°±å¯ä»¥è·å¾—æ‰€æœ‰ç›®çš„åœ°,è€Œ1~6åœ¨åŸç½‘é¡µ

æˆ‘ä»¬çš„ä»»åŠ¡å°±æ˜¯é¦–å…ˆçˆ¬å–æ‰€æœ‰çš„ä¸­å›½ç›®çš„åœ°(çš„é“¾æ¥),ç„¶åå†çˆ¬ç›®çš„åœ°é¡µé¢çš„æ™¯ç‚¹

# åŠ¨æ‰‹

## å®‰è£…Scrapy

(å·²ç»å®‰è£…äº†Python3 & pip3)

```shell
pip3 install scrapy
```

å®‰è£…å®Œæˆåå¯ä»¥åœ¨æ‰€éœ€ä½ç½®ç›´æ¥

```shell
scrapy startproject maotuying
```

æ–°å»ºå·¥ç¨‹

## é…ç½®items

åœ¨`maotuying`->`maotuying`->`items.py`ä¸­é…ç½®éœ€è¦é€šè¿‡Pipelineå­˜å‚¨çš„å†…å®¹(å³çˆ¬è™«è·å–çš„å¯¹è±¡)

å…ˆè€ƒè™‘åšç¬¬ä¸€çº§çˆ¬è™«(ç›®çš„åœ°HTML),æ‰€ä»¥

```python
class MaotuyingItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    des_name = scrapy.Field()
    des_html = scrapy.Field()
    pass
```

## åˆ›å»ºçˆ¬è™«

```shell
scrapy genspider sites www.tripadvisor.cn
```

`deshtmls`æ˜¯çˆ¬è™«ä»»åŠ¡çš„åç§°,` www.tripadvisor.cn`æ˜¯domain name

> é€šå¸¸domain.nameéƒ½æ˜¯ä¸å¸¦`www`çš„,ä½†æ˜¯çŒ«é€”é¹°ç½‘ç«™çš„éƒ½å¸¦,æŸ¥é˜…ä¸€ä¸‹è®ºå›å‘ç°
>
> The "www." convention is redundant, old fashioned and ... (IMO) ... ugly. Most places who use "www.example.com" will also have a server at "example.com", and one will redirect to the other as appropriate.
>
> æœç„¶å°è¯•ä¸€ä¸‹åœ¨æµè§ˆå™¨è¾“å…¥`http://tripadvisor.cn`ä¼šç›´æ¥è·³è½¬æˆ`https://www.tripadvisor.cn`

> å¦: httpsçš„sä»£è¡¨`secure`,æ˜¯ä½¿ç”¨SSLåŠ å¯†ä¼ è¾“çš„HTTP,æ›´åŠ å®‰å…¨(åŒæ ·çš„,ä½¿ç”¨httpsçš„ç½‘ç«™ä¼šè‡ªåŠ¨å°†`http`åè®®æ›´æ­£ä¸º`https`)

å»ºç«‹çˆ¬è™«å,æ‰“å¼€`sites.py`æ–‡ä»¶,å°†`start_urls`é»˜è®¤ä¸º`http://www.tripadvisor.cn`ä¿®æ”¹æˆäº†

```python
class DeshtmlsSpider(scrapy.Spider):
    name = 'deshtmls'
    allowed_domains = ['www.tripadvisor.cn']
    start_urls = ['https://www.tripadvisor.cn/Tourism-g294211-China-Vacations.html']

    def parse(self, response):
        pass
```

##  æ£€æŸ¥æ‰€éœ€é¡µé¢å…ƒç´ 

ä½¿ç”¨`Chrome`çš„`Developer Tools`

![](https://oliveds-1258728895.cos.ap-beijing.myqcloud.com/Screen%20Shot%202019-04-15%20at%2010.56.56%20AM.png)

## å°è¯•çˆ¬å–

å°è¯•çˆ¬å–åå‘ç°responseä¸ºç©º,ä½¿ç”¨scrapy shellè¿›è¡Œè°ƒè¯•,å‘ç°

```shell
% scrapy shell 'https://www.tripadvisor.cn/Tourism-g294211-China-Vacations.html' --nolog
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x1065289e8>
[s]   item       {}
[s]   request    <GET https://www.tripadvisor.cn/Tourism-g294211-China-Vacations.html>
[s]   settings   <scrapy.settings.Settings object at 0x106528748>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
In [1]: view(response)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-1-c68f42c3ba27> in <module>
----> 1 view(response)

/usr/local/lib/python3.7/site-packages/scrapy/utils/response.py in open_in_browser(response, _openfunc)
     68     from scrapy.http import HtmlResponse, TextResponse
     69     # XXX: this implementation is a bit dirty and could be improved
---> 70     body = response.body
     71     if isinstance(response, HtmlResponse):
     72         if b'<base' not in body:

AttributeError: 'NoneType' object has no attribute 'body'
```

æ‰¾åˆ°ä¸€ä¸ªè§£ç­”:

<https://stackoverflow.com/questions/43973898/scrapy-shell-return-without-response>

å°†setting.pyä¸­ä¿®æ”¹ä¸º

```
ROBOTSTXT_OBEY=False
```

ä¿®æ”¹å,å¯ä»¥åœ¨shell çˆ¬å–çš„ç•Œé¢ä¸­

```
In [4]: response.xpath('//div[@class="popularCities"]/a/@href').extract()
Out[4]:
['/Tourism-g294217-Hong_Kong-Vacations.html',
 '/Tourism-g294212-Beijing-Vacations.html',
 '/Tourism-g308272-Shanghai-Vacations.html',
 '/Tourism-g297463-Chengdu_Sichuan-Vacations.html',
 '/Tourism-g297415-Shenzhen_Guangdong-Vacations.html',
 '/Tourism-g298557-Xi_an_Shaanxi-Vacations.html']
```

# æ­£å¼çˆ¬å–

## çˆ¬å–é€»è¾‘

1. çˆ¬å–ä¸­å›½æ‰€æœ‰æ™¯ç‚¹çš„URL,å¹¶å­˜å‚¨åˆ°MySQLæ•°æ®åº“ä¸­
2. ä¾æ¬¡çˆ¬å–æ‰€æœ‰æ™¯ç‚¹çš„ä¸­æ–‡è¯„è®º

### çˆ¬å–ä¸­å›½æ‰€æœ‰æ™¯ç‚¹çš„URL

å…·ä½“åˆå¯ä»¥åˆ†ä¸º:

1. `start_requests`æ–¹æ³•æ„é€ åˆå§‹è¯·æ±‚(ç›®çš„åœ°)çš„request

   éœ€è¦è®¾ç½®`{offset}`å€¼è¯·æ±‚æ‰€æœ‰ç›®çš„åœ°ç½‘é¡µ

2. `parse_des`æ–¹æ³•è§£æç›®çš„åœ°çš„URL

3. `parse_sites_firstpage`æ–¹æ³•è§£æç›®çš„åœ°çš„æ™¯ç‚¹åˆ—è¡¨çš„é¡µæ•°,å¹¶æ„é€ ä¸‹ä¸€æ­¥è¯·æ±‚æ™¯ç‚¹åˆ—è¡¨çš„URL

   åŒæ ·éœ€è¦è®¾ç½®`{pages}`å€¼

4. `parse_sites_1page`è§£ææ™¯ç‚¹çš„åç§°å’ŒURL,å­˜å…¥MySQL

ä»£ç å¦‚ä¸‹:

```python
# -*- coding: utf-8 -*-
import scrapy
from maotuying.items import MaotuyingItem

class SitesSpider(scrapy.Spider):
    name = 'sites'
    allowed_domains = ['www.tripadvisor.cn']
    # start_urls = ['http://www.tripadvisor.cn/']
    '''
    ä¸¤å±‚çˆ¬è™«,å…ˆçˆ¬å–æ‰€æœ‰ç›®çš„åœ°çš„URL,å†åœ¨æ”¹URLä¸‹çˆ¬å–æ‰€æœ‰æ™¯ç‚¹
    parseå…ˆå°†ç›®çš„åœ°htmlè§£æå‡ºæ¥,ç„¶åè°ƒç”¨parse_sites
    parse_des å°†ç›®çš„åœ°çš„ä¸­çš„æ‰€æœ‰æ™¯ç‚¹URLçˆ¬å–ä¸‹æ¥
    parse_sites å°†æ¯ä¸ªæ™¯ç‚¹çš„å…·ä½“å†…å®¹çˆ¬å–å¹¶å­˜å‚¨ä¸‹æ¥
    '''
    custom_settings = {
        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',# å…è®¸çˆ¬å–é‡å¤çš„é¡µé¢ å› ä¸ºæˆ‘çš„ç¨‹åºä¸­å¯¹ç›®çš„åœ°é¦–é¡µå…ˆè§£æäº†pagesé¡µæ•°,æ‰€ä»¥ä¼šé‡å¤ä¸€æ¬¡!!å¦åˆ™çˆ¬ç¬¬ä¸€é¡µæ™¯ç‚¹å¤±è´¥
        'ITEM_PIPELINES' : {'maotuying.pipelines.MaotuyingPipeline': 300,}
    }

    # é€šå¸¸æƒ…å†µä¸‹ç»™å‡º start_urls å³å¯å¼€å§‹çˆ¬è™«,ä½†æ­¤å¤„éœ€è¦æ„é€ ,ä¸“é—¨è®¾ç½®ä¸€ä¸ªå‡½æ•°ç¡®å®šéœ€è¦è¢«çˆ¬å–çš„URL,å­˜åˆ°requestsä¸­,é€ä¸ªè¢«è¯·æ±‚
    def start_requests(self):
        requests = []
        url = '''https://www.tripadvisor.cn/TourismChildrenAjax?geo=294211&offset={offset}&desktop=true'''
        for i in range(212):# é€šè¿‡å°è¯•,å‘ç°å…±0~211é¡µ ä¹Ÿå¯ä»¥whileå¾ªç¯ç›´åˆ°æ²¡æœ‰æ–°çš„ç›®çš„åœ°
            request = scrapy.Request(url.format(offset=i), callback=self.parse_des)
            # print(request)
            # print("start_requests",i)
            requests.append(request)
        return requests

    def parse_des(self, response): # è§£æç›®çš„åœ°é“¾æ¥
        des_s = response.xpath('//a/@href').extract()# é¡µé¢ä¸­æ‰€æœ‰açš„æ•°ç»„,å³æ‰€æœ‰ç›®çš„åœ°
        print("this is parse_des")
        for des in des_s:
            des_parts = des.split('-') # e.g. /Tourism-g298559-Hangzhou_Zhejiang-Vacations.html
            print(des_parts[2])
            # https://www.tripadvisor.cn/Attractions-g298559-Activities-Hangzhou_Zhejiang.html
            url = 'https://www.tripadvisor.cn/Attractions-' + des_parts[1] + '-Activities-' + des_parts[2] + '.html'
            print("parse_des",url)
            yield scrapy.Request(url, self.parse_sites_firstpage)

    def parse_sites_firstpage(self, response):# è§£æç›®çš„åœ°çš„æ™¯ç‚¹é¡µæ•°
        # url = 'https://www.tripadvisor.cn/Attractions-' + des_parts[1] + '-Activities-oa' + str(i*30) + '-' + des_parts[2] + '.html'
        desurl = response.xpath('//li[@data-element=".masthead-dropdown-attractions"]/a/@href').extract()# é¡µé¢ä¸­æ‰€æœ‰é¡µç çš„æ•°ç»„
        # desurl = response.url
        # e.g. /Attractions-g294217-Activities-Hong_Kong.html
        print(desurl[0])
        des_parts = desurl[0].split('-')
        # https://www.tripadvisor.cn/Attractions-g294217-Activities-Hong_Kong.html
        # url = 'https://www.tripadvisor.cn/Attractions-' + des_parts[1] + '-Activities-' + des_parts[3] #+ '.html'
        # print("only parse page 1",url)
        try:
            page_s = response.xpath('//div[@class="pageNumbers"]/a/@data-page-number').extract()# é¡µé¢ä¸­æ‰€æœ‰é¡µç çš„æ•°ç»„
            pages = int(page_s[-1])# æœ€åä¸€ä¸ªé¡µç å°±æ˜¯æœ€å¤§é¡µç 
        except:
            pages = 1;
        print("this is parse_sites_pages") #,pages
        for i in range(pages):
            # print("this is i in range(pages)",i)
            url = 'https://www.tripadvisor.cn/Attractions-' + des_parts[1] + '-Activities-oa' + str(i*30) + '-' + des_parts[3] #+ '.html'
            print("this is i in range(pages)",i,url)
            yield scrapy.Request(url, self.parse_sites_1page)
 
    def parse_sites_1page(self, response):
        site_s = response.xpath('//div[@class="listing_title "]/a/@href').extract()# é¡µé¢ä¸­æ‰€æœ‰açš„æ•°ç»„
        sites_items = []
        print("this is parse_sites_1page",site_s)
        for site in site_s:
            # for example: /Attraction_Review-g298559-d386917-Reviews-West_Lake_Xi_Hu-Hangzhou_Zhejiang.html
            # /Attraction_Review-g297431-d501536-Reviews-Cangyan_Mountain-Shijiazhuang_Hebei.html
            site_parts = site.split('-')
            # if url is not None:
            url = 'https://www.tripadvisor.cn' + site
            print(site_parts[4],url)

            # å®ç°æ•°æ®åº“pipelineè¯»å†™
            item = MaotuyingItem()
            item['site_name'] = site_parts[4]
            item['site_url'] = url
            # TypeError: can only concatenate str (not "MaotuyingItem") to str
            # print("this is parse_sites's item" + item['site_url']) # æ£€æŸ¥itemæ˜¯å¦æˆåŠŸç”Ÿæˆ
            sites_items.append(item)
            # yield scrapy.Request(url, self.parse_site)
        return sites_items# å¿…é¡»return not pass å¦åˆ™des_sä¸­åç»­strä¸æ‰§è¡Œ(forå¾ªç¯åªæ‰§è¡Œä¸€æ¬¡)
```

### ä¾æ¬¡çˆ¬å–æ‰€æœ‰æ™¯ç‚¹çš„ä¸­æ–‡è¯„è®º

å…·ä½“æ­¥éª¤åˆä¸º:

1. `start_requests`æ–¹æ³•ä»MySQLä¸­å–å‡ºæ™¯ç‚¹çš„URL,æ„é€ åˆå§‹è¯·æ±‚

2. `parse_sites_firstpage`è§£ææ™¯ç‚¹çš„è¯„è®ºé¡µæ•°,,å¹¶æ„é€ ä¸‹ä¸€æ­¥è¯·æ±‚è¯„è®ºé¡µé¢çš„URL

   åŒæ ·éœ€è¦é…ç½®URLä¸­çš„`{pages}`å€¼

3. `parse_site_reviews`æ–¹æ³•è§£æè¯„è®ºçš„è¯¦ç»†å†…å®¹,å¹¶å­˜å‚¨åˆ°æ•°æ®åº“ä¸­

ä»£ç å¦‚ä¸‹:

```python
# -*- coding: utf-8 -*-
import scrapy
import pymysql
from maotuying.items import MaotuyingReviewItem

class SitedSpider(scrapy.Spider):
    name = 'sited'
    allowed_domains = ['www.tripadvisor.cn']
    # start_urls = ['http://www.tripadvisor.cn/']

    custom_settings = {
        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',# å…è®¸çˆ¬å–é‡å¤çš„é¡µé¢ å› ä¸ºæˆ‘çš„ç¨‹åºä¸­å¯¹ç›®çš„åœ°é¦–é¡µå…ˆè§£æäº†pagesé¡µæ•°,æ‰€ä»¥ä¼šé‡å¤ä¸€æ¬¡!!å¦åˆ™çˆ¬ç¬¬ä¸€é¡µæ™¯ç‚¹å¤±è´¥
        'ITEM_PIPELINES' : {'maotuying.pipelines.MaotuyingReviewPipeline': 300,}
    }

    def start_requests(self):
        # ä»æ•°æ®åº“ä¸­æ‰¾åˆ°æ‰€æœ‰æ™¯ç‚¹çš„URL
        self.connect = pymysql.connect(
            host = 'localhost',
            port = 3306,
            user = 'dsjk',
            password = 'dsjk',
            database = 'scrapy_sies',  
            charset='utf8',
            use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)
        sql = "select site_url from sites_all_url_copy4"# table: sites_all_url_copy4
        self.cursor.execute(sql)
        results = self.cursor.fetchall()
        print(len(results)," sites total.")
        requests = []
        i = 0
        for url in results:
            # i += 1
            # print(url[0])
            request = scrapy.Request(url[0], callback=self.parse_sites_firstpage)#Request(url, callback=self.parse_review, meta={'movieId': movieId}, dont_filter=True)
            requests.append(request)
            # if i > 100:
            #     break
        return requests

    def parse_sites_firstpage(self, response):# è§£æç›®çš„åœ°çš„æ™¯ç‚¹é¡µæ•°
        desurl = response.url
        # print(desurl) #https://www.tripadvisor.cn/Attraction_Review-g298557-d1801584-Reviews---Xiangzi_Temple-Xi_an_Shaanxi.html
        des_parts = desurl.split('-')
        try:
            pages = response.xpath('//div[@class="pageNumbers"]/a/@data-page-number').extract()# é¡µé¢ä¸­æ‰€æœ‰é¡µç çš„æ•°ç»„
            # print("pages is",pages)
            page = int(pages[-1])
            # print("transfer pages to int",pages)
        except:
            page = 1;
        for i in range(page):
            url = des_parts[0] + '-' + des_parts[1] + '-' + des_parts[2] + '-' + des_parts[3] + '-or' + str(i*10) + '-' + des_parts[4] + '-' + des_parts[5]#+ '.html'
            ##print("this is i in range(pages)",i,url)
#             yield scrapy.Request(
#                 url,
#                 formname = language,
#                 formdata = {
#                     'language': 'ALL', # or any other year value
#                 }
#                 callback=self.parse_site_reviews
# )
            yield scrapy.Request(url, self.parse_site_reviews)


    def parse_site_reviews(self, response): # è§£æç›®çš„åœ°è¯¦æƒ…
        desurl = response.url
        # print(desurl) #https://www.tripadvisor.cn/Attraction_Review-g298557-d1801584-Reviews---Xiangzi_Temple-Xi_an_Shaanxi.html
        des_parts = desurl.split('-')

        review_quote = response.xpath('//span[@class="noQuotes"]').xpath('string(.)').extract()
        review_detail = response.xpath('//p[@class="partial_entry"]').xpath('string(.)').extract()
        review_user = response.xpath('//div[@class="info_text"]/div[1]').xpath('string(.)').extract()# div[1]è¡¨ç¤ºæå–å…¶ä¸­çš„ç¬¬ä¸€ä¸ªdiv
        review_time = response.xpath('//span[@class="ratingDate"]/@title').extract()
        review_url = response.xpath('//div[@class="quote"]/a/@href').extract()    
        review_items = []
        # print("this is parse_site_reviews",desurl)
        for i in range(len(review_quote)):
            item = MaotuyingReviewItem()
            item['review_site'] = des_parts[-2]# å€’æ•°ç¬¬äºŒä¸ºæ™¯ç‚¹åç§°
            try:
                url_r = review_url[i]
            except:
                url_r = ""
            item['review_url'] = url_r
            item['review_quote'] = review_quote[i]
            item['review_detail'] = review_detail[i]
            item['review_user'] = review_user[i]
            item['review_time'] = review_time[i]
            review_items.append(item)
        return review_items# å¿…é¡»return not pass å¦åˆ™des_sä¸­åç»­strä¸æ‰§è¡Œ(forå¾ªç¯åªæ‰§è¡Œä¸€æ¬¡)
```



## è¿æ¥MySQL

å‚è€ƒè¿™ç¯‡æ•™ç¨‹

<https://www.accordbox.com/blog/scrapy-tutorial-9-how-use-scrapy-item/>

æ³¨æ„è¿˜éœ€è¦

```shell
pip3 install mysqlclient
```

æ‰èƒ½æ‰§è¡Œdb_connect()

å…¶ä¸­`models.py`çš„å†…å®¹ä¸­çš„`QuoteDB`éœ€è¦ä¿®æ”¹ä¸ºè‡ªå·±çš„DB,ç„¶åæµ‹è¯•ä»£ç ä¸­çš„DBåç§°å’Œå­—æ®µä¹Ÿéœ€è¦æ ¹æ®è‡ªå·±çš„DBä¿®æ”¹. In my case æµ‹è¯•ä»£ç å’Œç»“æœä¸º

```
from sqlalchemy.orm import sessionmaker
   ...: from maotuying.models import SitesDB, db_connect, create_tab
   ...: le
   ...:
   ...: engine = db_connect()
   ...: create_table(engine)
   ...: Session = sessionmaker(bind=engine)
   ...:
   ...: session = Session()
   ...: quotedb = SitesDB()
   ...: quotedb.sites_name = "test name"
   ...: quotedb.sites_url = "test url"
   ...:
   ...: try:
   ...:     session.add(quotedb)
   ...:     session.commit()
   ...:
   ...:     #query again
   ...:     obj = session.query(SitesDB).first()
   ...:     print(obj.sites_name,obj.sites_url)
   ...: except:
   ...:     session.rollback()
   ...:     raise
   ...: finally:
   ...:     session.close()
   ...:
test name test url
```

åœ¨æ‰§è¡Œè¿™æ®µæµ‹è¯•ç¨‹åºæ—¶,å‡ºç°äº†lib `ImportError`,

```shell
ImportError: dlopen(/usr/local/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so, 2): Library not loaded: libcrypto.1.0.0.dylib
  Referenced from: /usr/local/lib/python3.7/site-packages/MySQLdb/_mysql.cpython-37m-darwin.so
  Reason: image not found
```

```shell
% sudo ln -s /usr/local/mysql-8.0.15-macos10.14-x86_64/lib/libcrypto.1.0.0.dylib /usr/local/lib/libcrypto.1.0.0.dylib
```

æˆ‘çš„è§£å†³æ–¹æ¡ˆæ˜¯æŠŠéœ€è¦çš„libé€ä¸ªlinkåˆ°äº†`/usr/local/lib/`ä¸‹. ~~å¦‚æœæœ‰æ›´å¥½çš„è§£å†³æ–¹æ¡ˆ,please let me know~~

*ä¹Ÿå¯ä»¥ä½¿ç”¨å¼ è€å¸ˆè¯¾ä»¶ä¸Š`pymysql`æ’ä»¶è¿›è¡Œè¿æ¥(éœ€è¦è‡ªå·±åˆ›å»ºæ•°æ®åº“è¡¨å’Œç¼–å†™insertè¯­å¥,ç¨å¾®éº»çƒ¦ä¸€ç‚¹)*

åˆ›å»ºæ•°æ®åº“å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­å¥

```
create table sites_trial(
   id INT NOT NULL AUTO_INCREMENT,
   site_name VARCHAR(100) DEFAULT NULL,
   site_url VARCHAR(300) DEFAULT NULL,
   PRIMARY KEY (id)
)ENGINE=InNoDB DEFAULT CHARSET=utf8;
```



##  

```
https://www.tripadvisor.cn/ShowUserReviews-g317092-d1754693-r254761540-ZHCN.html
```



# MySQLæ•°æ®åº“æœåŠ¡å™¨

## é˜¿é‡Œäº‘ECSæœåŠ¡å™¨

å› ä¸ºéœ€è¦å°†æ•°æ®åº“æä¾›ç»™å…¶ä»–åŒå­¦è®¿é—®,éœ€è¦æ­å»ºä¸€ä¸ªMySQLæœåŠ¡å™¨,æˆ‘ç”¨å­¦ç”Ÿä¼˜æƒ å¼€äº†ä¸€å°9.5å…ƒ/æœˆçš„ECSäº‘æœåŠ¡å™¨,é€šè¿‡å…¶å…¬ç½‘IP,å¯ä»¥è¿œç¨‹è®¿é—®

```
sudo ssh 47.93.238.102
```

ECSåˆšæ‹¿åˆ°åéœ€è¦

```
sudo apt-get update
apt undate
```

å®‰è£…MySQL-Serveræˆ‘æ ¹æ®é˜¿é‡Œäº‘å®˜æ–¹çš„æ•™ç¨‹é‡åˆ°äº†å¾ˆå¤šé”™è¯¯,åè€Œåæ¥æŒ‰ç…§

å®˜æ–¹: https://yq.aliyun.com/articles/654980

```
sudo apt-get install mysql-server
sudo service mysql start # æ³¨æ„Linuxä¸Šä¸æ˜¯mysqld!!!
ps -ef | grep mysql | grep -v grep # å‘ç°æœåŠ¡å·²ç»å¯åŠ¨äº†
```

## é…ç½®å®‰å…¨ç»„è§„åˆ™

éœ€è¦ç»™ECSé…ç½®å…è®¸å…¬ç½‘è®¿é—®å…¶3306ç«¯å£,å¦‚ä¸‹å›¾æ‰€ç¤º

![](https://oliveds-1258728895.cos.ap-beijing.myqcloud.com/%E5%AE%89%E5%85%A8%E7%BB%84%E8%A7%84%E5%88%99.png)

## æ•°æ®åº“è¿ç§»

ä¸€å¼€å§‹æˆ‘æŠŠæ•°æ®éƒ½å­˜åœ¨äº†æœ¬åœ°çš„MySQLä¸­,æ‰€ä»¥éœ€è¦è¿ç§»ä¸ŠECS

### å°†Databaseå­˜ä¸º .sqlæ–‡ä»¶

```
mysqldump -u dsjk -p scrapy_sies>scrapy_sies.sql
```

### å°†æ–‡ä»¶ä¸Šä¼ åˆ°ECS

```
scp /Users/oliveds/Documents/web/maotuying/scrapy_sies.sql root@47.93.238.102:/home/oliveds/
```

è¿™ä¸ªä»£ç åœ¨æœ¬æœºæ‰§è¡Œ,å‰é¢æ˜¯`æºIP`(çœç•¥)`æºè·¯å¾„+æ–‡ä»¶å` åé¢æ˜¯`ç›®æ ‡IP`(å³ECSçš„IP) `ç›®æ ‡è·¯å¾„`

æœç„¶å¯ä»¥åœ¨æˆ‘çš„ECSå‘½ä»¤è¡Œä¸­æŸ¥æ‰¾åˆ°å®ƒäº†

```
root@...:/home/oliveds# ls
mysql57-community-release-el7-11.noarch.rpm  scrapy_sies.sql
```

### ECSä¸­æ¢å¤Database

```
create database scrapy_sies; # åˆ›å»ºåŸåç§°çš„æ•°æ®åº“
use scrapy_sies; # è¿›å…¥
source /home/oliveds/scrapy_sies.sql # æ¢å¤
```



## é…ç½®MySQLæ•°æ®åº“å¯è¢«è¿œç¨‹è®¿é—®

### åˆ›å»ºæ–°ç”¨æˆ·

é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ–°ç”¨æˆ·,æä¾›ç»™åŒå­¦å’Œè€å¸ˆä»¬ä½¿ç”¨

```
mysql> create user dsjk identified by 'dsjk';
```

ç¬¬ä¸€ä¸ª`dsjk`ä¸ºç”¨æˆ·å,ç¬¬äºŒä¸ªä¸ºå¯†ç 

### é…ç½®ç”¨æˆ·æƒé™

```
mysql> grant all privileges on scrapy_sies.* to dsjk@'%';
Query OK, 0 rows affected (0.00 sec)
mysql> flush privileges;
mysql> exit;
```

è¿™é‡Œ`all`è¡¨ç¤ºèµ‹äºˆäº†æ‰€æœ‰æƒé™,åŒ…æ‹¬`SELECT`,`INSERT`,`UPDATE``ç­‰,ä¹Ÿå¯ä»¥åªèµ‹äºˆéƒ¨åˆ†

`scrapy_sies`æ˜¯databaseçš„åç§°,è¿™é‡Œåªèµ‹äºˆäº†æ­¤databaseä¸‹æ‰€æœ‰table(`.*`è¡¨ç¤ºæ‰€æœ‰),ä¹Ÿå¯ä»¥ä½¿ç”¨`*.*`èµ‹äºˆè¯¥ç”¨æˆ·è®¿é—®æ‰€æœ‰databaseçš„æƒé™,

`@'%'`ä¸­çš„`%`è¡¨ç¤ºdsjkå¯ä»¥åœ¨ä»»ä½•ipè®¿é—®,å¦‚æœæƒ³è¦é™åˆ¶åªæœ‰æŸä¸ªipå¯ä»¥è®¿é—®,å°†`'%'`æ¢æˆé‚£ä¸ªipåœ°å€(xxx.xxx.xxx.xxx or localhost)å°±å¯ä»¥äº†

```
vim /etc/mysql/mysql.conf.d/mysqld.cnf
# æ³¨é‡Šæ‰ bind_address è¡Œ æˆ–æ”¹æˆ0.0.0.0(å¾€ä¸‹æ»‘,æˆ‘ç¬¬ä¸€æ¬¡éƒ½æ²¡æœ‰æ‰¾åˆ°)
```

ç„¶åé‡å¯ä¸€ä¸‹ECSæœºå™¨

### è¿œç¨‹è®¿é—®-Navicat

ä½¿ç”¨`Navicat`è®¿é—®è¿œç¨‹çš„æ•°æ®åº“æ¯”è¾ƒæ–¹ä¾¿,å®‰è£…`Navicat for MySQL`,æ–°å»ºè¿æ¥,è¾“å…¥ip(ä¹Ÿå°±æ˜¯MySQLå®‰è£…æœºå™¨çš„IPåœ°å€,å¯ä»¥é€šè¿‡`ifconfig`æŸ¥çœ‹),port(é€šå¸¸æ˜¯3306),åˆšæ‰åˆ›å»ºçš„å¯ä»ä»»æ„ipè®¿é—®çš„æ–°ç”¨æˆ·(dsjk)çš„ç”¨æˆ·åå’Œå¯†ç ,å³å¯å»ºç«‹è¿æ¥

![](https://oliveds-1258728895.cos.ap-beijing.myqcloud.com/navicat-new-connection.png)

`Open Connection`å,å¯ä»¥çœ‹åˆ°æˆæƒäº†è¯¥ç”¨æˆ·è®¿é—®çš„`scrapy_sies`databaseä¸­çš„å†…å®¹

![](https://oliveds-1258728895.cos.ap-beijing.myqcloud.com/navicat-database.png)

# SideNote

## é‡å¤é¡µé¢çˆ¬å–

æˆ‘çš„ç¨‹åºéœ€è¦å…ˆå¯¹ç›®çš„åœ°é¡µé¢ä¸­çš„é¡µç é¡µç è¿›è¡Œæå–(è§£æç›®çš„åœ°çš„æ™¯ç‚¹é¦–é¡µ),ç„¶åå†è§£æç›®çš„åœ°æ™¯ç‚¹çš„æ¯ä¸ªé¡µé¢,è¿™æ ·å°±é€ æˆç›®çš„åœ°çš„æ™¯ç‚¹é¦–é¡µè¢«scrapyè§£æäº†ä¸¤æ¬¡,è€Œscrapyæ˜¯é»˜è®¤å»é‡çš„,æ‰€ä»¥æˆ‘ä¸€ç›´æ²¡æœ‰çˆ¬åˆ°æ¯ä¸ªç›®çš„åœ°é¦–é¡µçš„30ä¸ªæ™¯ç‚¹ğŸ˜­

è¿™ä¸ªé—®é¢˜å›°æ‰°äº†ä¸€æ•´å¤©,å› ä¸ºé—®é¢˜å®åœ¨ä¸å¤ªå¥½æ‰¾,æˆ‘åˆè¿™ä¹ˆæ··ä¹±,ä¸ç†Ÿæ‚‰,ä¸ºäº†æ‰¾è¿™ä¸ªbugæˆ‘å‡Œæ™¨3ç‚¹æ‰ç¡â€¦ç„¶å¹¶åµğŸ˜¡åæ¥æ‰å‘ç°æ˜¯å› ä¸ºè¢«å»é‡äº†,å¥½è ¢ğŸ˜

```
custom_settings = {
        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',# å…è®¸çˆ¬å–é‡å¤çš„é¡µé¢ å› ä¸ºæˆ‘çš„ç¨‹åºä¸­å¯¹ç›®çš„åœ°é¦–é¡µå…ˆè§£æäº†pagesé¡µæ•°,æ‰€ä»¥ä¼šé‡å¤ä¸€æ¬¡!!å¦åˆ™çˆ¬ç¬¬ä¸€é¡µæ™¯ç‚¹å¤±è´¥
    }
```

å¦‚ä¸Š,é…ç½®å…¶å¯ä»¥é‡å¤çˆ¬å–

## Chromeè¯»å–ç¼“å­˜

è¢«åçˆ¬è™«äº†,ä½†æ˜¯codeè¿˜è¦ç»§ç»­å†™,è¿™æ—¶å€™è®¿é—®ä¸äº†ç½‘ç«™äº†,äºæ˜¯åªèƒ½å…ˆçœ‹ç¼“å­˜çš„å†…å®¹äº†

è®¿é—®`chrome://chrome-urls/`,é€‰æ‹©`chrome://cache`,å°±å¯ä»¥æ‰¾ä¹‹å‰ç¼“å­˜çš„é¡µé¢äº†

## Spider's Custom Settings å¦™ç”¨

é’ˆå¯¹ä¸åŒçš„spideræœ‰ä¸åŒé…ç½®æ—¶,åˆ†åˆ«å†™åœ¨å…¶`custom_settings`ä¸­,è€Œä¸æ˜¯å†™åœ¨`project`->`settings.py`,å¯ä»¥é¿å…ç›¸äº’å½±å“,æ¯”å¦‚ä¸¤è€…ä½¿ç”¨ä¸åŒPipelineæ—¶,å¯ä»¥

```
custom_settings = {
        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
        'ITEM_PIPELINES' : {'maotuying.pipelines.MaotuyingReviewPipeline': 300,}
    }
```

## IPä»£ç†

è™½ç„¶é€šè¿‡ è®¾ç½®`request Header `è¯·æ±‚å¤´èƒ½å¤Ÿåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šé˜²æ­¢è¢«åçˆ¬,ä½†æ˜¯æ•°æ®é‡å¤ªå¤§çš„æ—¶å€™,ä¾ç„¶å¾ˆå®¹æ˜“ä¼šè¢«ç¦

è¿™æ—¶å€™å¦‚æœèƒ½æœ‰ä¸€ä¸ªIPæ± æ¥ä¸æ–­æ¢IPåœ°å€å°±æ›´åŠ é«˜æ•æ— å¿§äº†.å½“ç„¶,å…è´¹çš„IPæ± é‡Œèƒ½ç”¨çš„ä¸å¤š,æœ€å¥½è¿˜æ˜¯è´­ä¹°ä¸€ä¸ªIPä»£ç†

é…ç½®æ–¹æ³•è§è¿™ä¸ªæ•™ç¨‹:

<https://blog.csdn.net/u010978757/article/details/83409571>

# ç»“æœå±•ç¤º

Table: sites_all_url_copy4 (ä¸­å›½æ‰€æœ‰æ™¯ç‚¹)

![](https://oliveds-1258728895.cos.ap-beijing.myqcloud.com/sites_all_url_copy4.png)

Table: sites_all_review (æ‰€æœ‰ä¸­æ–‡æ™¯ç‚¹è¯„è®º)

![](https://oliveds-1258728895.cos.ap-beijing.myqcloud.com/sites_all_review.png)